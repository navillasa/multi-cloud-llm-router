# GPU-specific values for AWS LLM server deployment
# This file provides GPU-optimized configuration that extends values-aws.yaml

gpu:
  enabled: true

# Override CPU model with larger GPU-optimized model  
model:
  uri: "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_k_m.gguf"
  path: "/models/llama-2-7b-chat.gguf"
  storageSize: "20Gi"

# GPU-optimized LLM settings
llamacpp:
  ngl: -1  # Use all GPU layers
  flash_attn: true  # Enable for GPU
  ctx_size: 4096  # Larger context
  parallel: 8  # More parallel requests

# GPU cost override
cloud:
  costPerHour: 0.378  # g4dn.xlarge spot price

# Ingress for GPU endpoint
ingress:
  hosts:
    - host: aws-gpu.llm.yourdomain.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: llm-server-tls-aws-gpu
      hosts:
        - aws-gpu.llm.yourdomain.com
