# GPU-specific values for GCP LLM server deployment  
# This file provides GPU-optimized configuration that extends values-gcp.yaml

gpu:
  enabled: true

# Override CPU model with larger GPU-optimized model
model:
  uri: "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.q4_k_m.gguf"
  path: "/models/llama-2-7b-chat.gguf"
  storageSize: "20Gi"

# GPU-optimized LLM settings
llamacpp:
  ngl: -1  # Use all GPU layers
  flash_attn: true  # Enable for GPU
  ctx_size: 4096  # Larger context
  parallel: 8  # More parallel requests

# GPU cost override
cloud:
  costPerHour: 0.280  # n1-standard-4 + T4 preemptible

# Ingress for GPU endpoint
ingress:
  hosts:
    - host: gcp-gpu.llm.yourdomain.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: llm-server-tls-gcp-gpu
      hosts:
        - gcp-gpu.llm.yourdomain.com
