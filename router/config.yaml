server:
  port: 8080
  readTimeout: 30s
  writeTimeout: 120s
  idleTimeout: 60s

router:
  stickinessWindow: 60s
  healthCheckInterval: 30s
  maxLatencyMs: 5000
  maxQueueDepth: 10
  overheadFactor: 1.1
  metricsUpdateInterval: 30s
  # Routing strategy: "cost", "latency", "hybrid", "external_first", "cluster_first"
  routingStrategy: hybrid
  # Fallback to external providers when clusters are unhealthy
  enableExternalFallback: true
  # Prefer clusters for requests under this cost threshold ($/1K tokens)
  clusterCostThreshold: 0.01

clusters:
  - name: aws-us-west-2
    endpoint: https://aws.llm.yourdomain.com
    region: us-west-2
    provider: aws
    costPerHour: 0.0928  # t3.large on-demand price
    authType: hmac
    sharedSecret: your-shared-secret-here
    
  - name: gcp-us-central1
    endpoint: https://gcp.llm.yourdomain.com
    region: us-central1
    provider: gcp
    costPerHour: 0.0950  # n1-standard-2 price
    authType: hmac
    sharedSecret: your-shared-secret-here
    
  - name: azure-eastus
    endpoint: https://azure.llm.yourdomain.com
    region: eastus
    provider: azure
    costPerHour: 0.0968  # Standard_D2s_v3 price
    authType: hmac
    sharedSecret: your-shared-secret-here

# External LLM providers (OpenAI, Claude, Gemini)
externalProviders:
  - name: openai
    type: openai
    enabled: true
    apiKey: "${OPENAI_API_KEY}"
    defaultModel: gpt-3.5-turbo
    rateLimit:
      requestsPerMinute: 3500
      tokensPerMinute: 90000
      burstMultiplier: 1.5
    
  - name: claude
    type: claude
    enabled: true
    apiKey: "${ANTHROPIC_API_KEY}"
    defaultModel: claude-3-haiku-20240307
    rateLimit:
      requestsPerMinute: 1000
      tokensPerMinute: 100000
      burstMultiplier: 1.2
      
  - name: gemini
    type: gemini
    enabled: true
    apiKey: "${GEMINI_API_KEY}"
    defaultModel: gemini-1.5-flash
    rateLimit:
      requestsPerMinute: 1500
      tokensPerMinute: 32000
      burstMultiplier: 1.3
